%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{graphicx}

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage[procnames]{listings}

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{BRSU} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Neural Networks\\Assignment 5 \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Bastian Lang} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

\newpage

This report contains the summary, ex3.1 and ex3.2 without any programming/plotting.

\section{Outline}

\begin{itemize}
	\item Introduction
	\begin{itemize}
		\item Three distinct characteristics of MLP:
		\begin{itemize}
			\item Model of each neuron includes nonlinear activation function that is differentiable everywhere
			\item One or more layers of hidden units
			\item High degree of connectivity
		\end{itemize}
		\item Organization of the chapter
	\end{itemize}
	
	
	\item Some Preliminaries
	\begin{itemize}
		\item Signals in an MLP:
		\begin{itemize}
			\item Function Signals: Propagating forward through the network
			\item Error Signals: Propagating backwards through the network
		\end{itemize}
		\item Computations for each neuron:
		\begin{itemize}
			\item Function signal
			\item Gradient vector of error surface
		\end{itemize}
		\item Notation
	\end{itemize}
	
	
	\item Back-Propagation Algorithm
	\begin{itemize}
		\item Error signal at output neuron: $e_j(n) = d_j(n) - y_j(n)$
		\item Instantaneous value of total error energy: $\xi (n) = 0.5\sum_{j\epsilon C} e_j^2(n)$
		\item Induced Local Field $v_j(n) = \sum_{i=0}^{m}w_{ji}(n)y_i(n)$
		\item Function signal $y_i(n) = \phi _j(v_j(n))$
		\item Derivation of delta rule
		\item Local gradient for output neuron: $\delta _j(n) = e_j(n)\phi _j^\prime(v_j(n))$
		\item Local gradient for hidden neuron: $\delta _j(n) = \phi _j^\prime (v_j(n))\sum_{k}\delta_k(n)w_{kj}(n)$
		\item Weight correction: $\Delta w_{ji}(n) = \eta * \delta _j(n) * y_i(n)$
		\item Two passes of Computation
		\begin{itemize}
			\item Forward pass: Compute net output
			\item Backward pass: Adjust weights according to error
		\end{itemize}
		\item Activation Function
		\begin{itemize}
			\item Logistic Function
			\item Hyperbolic tangent function
		\end{itemize}
		\item Rate of Learning
		\begin{itemize}
			\item Including momentum term for stability
		\end{itemize}
		\item Sequential and Batch Modes of Training
		\begin{itemize}
			\item Epoch: Complete presentation of training set
			\item Sequential Mode of back-propagation learning: Update weights directly
			\item Batch Mode of back-propagation learning: Update weights only after an epoch
		\end{itemize}
		\item Summary of the Back-Propagation Algorithm
		\begin{itemize}
			\item Cycle for sequential updating:
			\begin{itemize}
				\item Initialization
				\item Presentations of Training Examples
				\item Forward Computation
				\item Backward Computation
				\item Iteration
			\end{itemize}
		\end{itemize}
	\end{itemize}
	
	\item Summary of the back-propagation algorithm
	
	
	\item XOR problem
	\begin{itemize}
		\item Special case of classifying points in the unit hypercube
		\item Elemental perceptron cannot solve XOR
		\item Example model using McCulloch-Pits neurons
	\end{itemize}
	
	
	\item Heuristics for making the back-propagation algorithm perform better
	\begin{itemize}
		\item Methods to improve back-prop
		\begin{itemize}
			\item Sequential versus batch update
			\item Maximizing information content
			\begin{itemize}
				\item Use of example that results in largest training error
				\item Use of example that is radically different than previous ones
			\end{itemize}
			\item Activation Function
			\begin{itemize}
				\item Better antisymmetric than non-symmetric
			\end{itemize}
			\item Target values
			\begin{itemize}
				\item Offset of target values to lie within function range
			\end{itemize}
			\item Normalizing the inputs
			\item Initialization
			\item Learning from hints
			\item Learning rates
		\end{itemize}
	\end{itemize}
\end{itemize}

\section{MLP for XOR}

I implemented the back propagation algorithm for the fixed 2-2-1-mlp after my other solutions were not working. But it turned out that my solution still is not able to adjust the weights correctly. I tried different learning rates and I even used 0.99 and 0.01 instead of 0 and 1 as desired outputs.\\
Following is the python code.\\
I started by implementing an OO solution using python, but python seemed to be doing some unexpected things, so I switched to Java and implemented backprop for dynamically sized MLPs. But as with the fixed solution it does not seem to work. I tried random sequential learning as well. I add the Java code as well. Because of these problems I did not spend any time on exercise 3.

\subsection{Python}
\begin{lstlisting}
# -*- coding: utf-8 -*-
"""
Created on Sat Nov 14 19:03:39 2015

@author: bastian
"""
import numpy as np

f = lambda x : 1 / ( 1 + np.exp(-x))
f_d = lambda x : f(x) * (1 - f(x))

class MLP:
    
    def __init__(self, initial_weights):
        # input to hidden
        self.w31 = initial_weights
        self.w41 = initial_weights
        self.w32 = initial_weights
        self.w42 = initial_weights
        # hidden to output
        self.w53 = initial_weights
        self.w54 = initial_weights
        # bias
        self.w30 = initial_weights
        self.w40 = initial_weights
        self.w50 = initial_weights
    
    def propagate(self, net_input):
        # input
        self.output_1 = net_input[0]
        self.output_2 = net_input[1]
        
        # hidden
        self.input_3 = self.w30+self.w31*self.output_1 + self.w32*self.output_2
        self.output_3 = f(self.input_3)
        self.input_4 = self.w40 + self.w41*self.output_1 + self.w42*self.output_2
        self.output_4 = f(self.input_4)
        
        #output
        self.input_5 = self.w50+self.w53*self.output_3 + self.w54*self.output_4
        self.output_5 = f(self.input_5)
        
        return self.output_5
        
    def backpropagate(self, net_input, desired_output, learning_rate):
        net_output = self.propagate(net_input)
        error = desired_output - net_output
        
        self.delta_5 = f_d(self.input_5) * error
        self.delta_4 = f_d(self.input_4) * self.w54 * self.delta_5
        self.delta_3 = f_d(self.input_3) * self.w53 * self.delta_5

        self.w50 = self.w50 + learning_rate*self.delta_5
        self.w53 = self.w53 + learning_rate*self.delta_5*self.output_3
        self.w54 = self.w54 + learning_rate*self.delta_5*self.output_4
        
        self.w40 = self.w40 + learning_rate * self.delta_4
        self.w42 = self.w42 + learning_rate * self.delta_4*self.output_2
        self.w41 = self.w41 + learning_rate * self.delta_4*self.output_1
        
        self.w30 = self.w30 + learning_rate * self.delta_3
        self.w32 = self.w32 + learning_rate * self.delta_3*self.output_2
        self.w31 = self.w31 + learning_rate * self.delta_3*self.output_1

    
mlp = MLP(0.5)
print mlp.propagate((1,1))
mlp.backpropagate((1,1),0.01, 0.1)

learning_rate = 0.1
for i in range(1000):
    mlp.backpropagate((1,1), 0.01, learning_rate)
    mlp.backpropagate((0,1), 0.99, learning_rate)
    mlp.backpropagate((1,0), 0.99, learning_rate)
    mlp.backpropagate((0,0), 0.01, learning_rate)
print mlp.propagate((0,0))
print mlp.propagate((1,0))
print mlp.propagate((0,1))
print mlp.propagate((1,1))
\end{lstlisting}

\subsection{Java}
\begin{lstlisting}
package model;

import java.util.ArrayList;
import java.util.LinkedList;
import java.util.List;

public class MLP {

    private final List<Neuron> inputLayer = new LinkedList<Neuron>();
    private final List<Neuron> hiddenLayer = new LinkedList<Neuron>();
    private final List<Neuron> outputLayer = new LinkedList<Neuron>();
    private final Neuron bias = new BiasNeuron();

    public MLP(int numberInputNeurons, int numberHiddenNeurons, int numberOutputNeurons, double initialWeight) {

        createInputLayer(numberInputNeurons);
        createFullyConnectedLayer(inputLayer, hiddenLayer, numberHiddenNeurons, initialWeight);
        createFullyConnectedLayer(hiddenLayer, outputLayer, numberOutputNeurons, initialWeight);

    }

    private void createInputLayer(int numberInput) {
        for (int i = 0; i < numberInput; i++) {
            Neuron inputNeuron = new InputNeuron();
            inputLayer.add(inputNeuron);
        }
    }

    private void createFullyConnectedLayer(List<Neuron> originLayer, List<Neuron> targetLayer, int layerSize,
            double initialWeight) {
        for (int i = 0; i < layerSize; i++) {
            Neuron targetNeuron = new StandardNeuron();
            for (Neuron originNeuron : originLayer) {
                Connection connection = new Connection(originNeuron, targetNeuron, initialWeight);
                originNeuron.addOutgoingConnection(connection);
                targetNeuron.addIncomingConnection(connection);
            }
            Connection connection = new Connection(bias, targetNeuron, initialWeight);
            targetNeuron.addIncomingConnection(connection);
            targetLayer.add(targetNeuron);
        }
    }

    public List<Double> propagate(List<Double> input) {
        for (int i = 0; i < inputLayer.size(); i++) {
            Neuron inputNeuron = inputLayer.get(i);
            inputNeuron.setLastInducedLocalField(input.get(i));
            inputNeuron.activate();
        }

        propagateThroughLayer(hiddenLayer);
        propagateThroughLayer(outputLayer);

        ArrayList<Double> result = new ArrayList<Double>();
        for (Neuron neuron : outputLayer) {
            result.add(neuron.getLastOutput());
        }
        return result;
    }

    private void propagateThroughLayer(List<Neuron> layer) {
        for (Neuron neuron : layer) {
            neuron.computeInducedLocalField();
            neuron.activate();
        }
    }

    public void backpropagate(List<Double> input, List<Double> desiredOutput, double learningRate) {
        List<Double> netOutput = propagate(input);
        // Compute delta for output neurons and change incoming weights
        for (int i = 0; i < outputLayer.size(); i++) {
            double error = desiredOutput.get(i) - netOutput.get(i);
            Neuron neuron = outputLayer.get(i);
            neuron.setLastDelta(error * neuron.derivative());
            changeIncomingWeightsForNeuron(learningRate, neuron);
        }

        // Compute delta for hidden neurons and change their incoming weights
        for (Neuron neuron : hiddenLayer) {
            double summedDelta = 0;
            for (Connection outgoingConnection : neuron.getOutgoingConnections()) {
                summedDelta += outgoingConnection.getWeight() * outgoingConnection.getTargetNeuron().getLastDelta();
            }
            neuron.setLastDelta(neuron.derivative() * summedDelta);
            changeIncomingWeightsForNeuron(learningRate, neuron);
        }
    }

    private void changeIncomingWeightsForNeuron(double learningRate, Neuron neuron) {
        for (Connection incomingConnection : neuron.getIncomingConnections()) {
            double weightChange = neuron.getLastDelta() * learningRate
                    * incomingConnection.getOriginNeuron().getLastOutput();
            incomingConnection.setWeight(incomingConnection.getWeight() + weightChange);
        }
    }

    @Override
    public String toString() {
        StringBuilder result = new StringBuilder();
        result.append("MLP:\nInput Layer:\n");
        for (Neuron neuron : inputLayer) {
            result.append(neuron.toString() + "\n");
        }
        result.append("\nHidden Layer:\n");
        for (Neuron neuron : hiddenLayer) {
            result.append(neuron.toString() + "\n");
        }
        result.append("\nOutput Layer:\n");
        for (Neuron neuron : outputLayer) {
            result.append(neuron.toString() + "\n");
        }
        return result.toString();
    }
}
package model;

import java.util.LinkedList;
import java.util.List;

public abstract class Neuron {

    private double lastInducedLocalField;
    private double lastDelta;
    private double lastOutput;
    private final List<Connection> outgoingConnections;
    private final List<Connection> incomingConnections;

    public Neuron() {
        outgoingConnections = new LinkedList<Connection>();
        incomingConnections = new LinkedList<Connection>();
    }

    public void addOutgoingConnection(Connection connection) {
        outgoingConnections.add(connection);
    }

    public void addIncomingConnection(Connection connection) {
        incomingConnections.add(connection);
    }

    public List<Connection> getIncomingConnections() {
        return incomingConnections;
    }

    public List<Connection> getOutgoingConnections() {
        return outgoingConnections;
    }

    public double getLastDelta() {
        return lastDelta;
    }

    public double getLastOutput() {
        return lastOutput;
    }

    public double getLastInducedLocalField() {
        return lastInducedLocalField;
    }

    public abstract double activate();

    public double computeInducedLocalField() {
        double result = 0;
        for (Connection connection : incomingConnections) {
            result += connection.getWeight() * connection.getOriginNeuron().getLastOutput();
        }
        setLastInducedLocalField(result);
        return result;
    }

    @Override
    public String toString() {
        StringBuilder result = new StringBuilder();
        result.append(String.format("Number of incomming connections: %d. ", getIncomingConnections().size()));
        result.append("weights:[");
        for (Connection connection : incomingConnections) {
            result.append(connection.getWeight() + " ");
        }
        result.append("]");
        result.append(String.format("Number of outgoing connections: %d. ", getOutgoingConnections().size()));
        return result.toString();
    }

    public void setLastOutput(double lastOutput) {
        this.lastOutput = lastOutput;
    }

    public void setLastDelta(double lastDelta) {
        this.lastDelta = lastDelta;
    }

    public void setLastInducedLocalField(double lastInducedLocalField) {
        this.lastInducedLocalField = lastInducedLocalField;
    }

    public abstract double derivative();
}
package model;

public class InputNeuron extends Neuron {

    @Override
    public String toString() {
        StringBuilder result = new StringBuilder();
        result.append("Input Neuron: ");
        result.append(super.toString());
        return result.toString();
    }

    @Override
    public double activate() {
        setLastOutput(getLastInducedLocalField());
        return getLastOutput();
    }

    @Override
    public double derivative() {
        System.out.println("Derivative of input neuron should never be needed.");
        return 0;
    }
}
package model;

public class StandardNeuron extends Neuron {

    @Override
    public double activate() {
        double activation = computeActivation();
        setLastOutput(activation);
        return activation;
    }

    private double computeActivation() {
        double activation = 1.0 / (1 + Math.exp(getLastInducedLocalField() * (-1)));
        return activation;
    }

    @Override
    public String toString() {
        StringBuilder result = new StringBuilder();
        result.append("Standard Neuron: ");
        result.append(super.toString());
        return result.toString();
    }

    @Override
    public double derivative() {
        return computeActivation() * (1 - computeActivation());
    }

}
package model;

public class BiasNeuron extends Neuron {

    public BiasNeuron() {
        setLastInducedLocalField(1);
        setLastOutput(1);
    }

    @Override
    public double activate() {
        setLastOutput(1);
        return 1;
    }

    @Override
    public String toString() {
        StringBuilder result = new StringBuilder();
        result.append("Bias Neuron: ");
        result.append(super.toString());
        return result.toString();
    }

    @Override
    public double derivative() {
        System.out.println("Derivative for bias should never be needed");
        return 0;
    }

}
package model;

public class Connection {

    private Neuron originNeuron;
    private Neuron targetNeuron;
    private double weight;

    public Connection(Neuron originNeuron, Neuron targetNeuron, double initialWeight) {
        this.originNeuron = originNeuron;
        this.targetNeuron = targetNeuron;
        this.weight = initialWeight;
    }

    public Neuron getOriginNeuron() {
        return originNeuron;
    }

    public double getWeight() {
        return weight;
    }

    public Neuron getTargetNeuron() {
        return targetNeuron;
    }

    public void setOriginNeuron(Neuron originNeuron) {
        this.originNeuron = originNeuron;
    }

    public void setTargetNeuron(Neuron targetNeuron) {
        this.targetNeuron = targetNeuron;
    }

    public void setWeight(double weight) {
        this.weight = weight;
    }

}
import java.util.Arrays;
import java.util.List;

import model.MLP;

/**
 * 
 * @author bastian
 * 
 */
public class Main {
    public static void main(String[] args) {
        MLP mlp = new MLP(2, 2, 1, 0.5);
        System.out.println(mlp);
        List<Double> input = Arrays.asList(0.0, 0.0);
        System.out.println(mlp.propagate(input));

        for (int i = 0; i < 10000; i++) {
            mlp.backpropagate(Arrays.asList(0.0, 0.0), Arrays.asList(0.01), 0.1);
            mlp.backpropagate(Arrays.asList(1.0, 0.0), Arrays.asList(0.99), 0.1);
            mlp.backpropagate(Arrays.asList(0.0, 1.0), Arrays.asList(0.99), 0.1);
            mlp.backpropagate(Arrays.asList(1.0, 1.0), Arrays.asList(0.01), 0.1);
        }

        System.out.println(mlp);
        System.out.println(mlp.propagate(Arrays.asList(0.0, 0.0)));
        System.out.println(mlp.propagate(Arrays.asList(1.0, 0.0)));
        System.out.println(mlp.propagate(Arrays.asList(0.0, 1.0)));
        System.out.println(mlp.propagate(Arrays.asList(1.0, 1.0)));
    }
}

\end{lstlisting}



\end{document}